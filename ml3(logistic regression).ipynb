{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f79c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a923c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdf15ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n  Mathematical Statistics\" (John Wiley, NY, 1950).\\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n  Structure and Classification Rule for Recognition in Partially Exposed\\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n  on Information Theory, May 1972, 431-433.\\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n  conceptual clustering system finds 3 classes in the data.\\n- Many, many more ...\\n\\n|details-end|',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3cf5471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75e965b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target']   #1-setosa, 2-versicolour, 3-virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "755e1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=iris['data'][:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b749258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.3],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.1],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.1],\n",
       "       [0.1],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.4],\n",
       "       [0.3],\n",
       "       [0.3],\n",
       "       [0.3],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.2],\n",
       "       [0.5],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.1],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.1],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.3],\n",
       "       [0.3],\n",
       "       [0.2],\n",
       "       [0.6],\n",
       "       [0.4],\n",
       "       [0.3],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [1.4],\n",
       "       [1.5],\n",
       "       [1.5],\n",
       "       [1.3],\n",
       "       [1.5],\n",
       "       [1.3],\n",
       "       [1.6],\n",
       "       [1. ],\n",
       "       [1.3],\n",
       "       [1.4],\n",
       "       [1. ],\n",
       "       [1.5],\n",
       "       [1. ],\n",
       "       [1.4],\n",
       "       [1.3],\n",
       "       [1.4],\n",
       "       [1.5],\n",
       "       [1. ],\n",
       "       [1.5],\n",
       "       [1.1],\n",
       "       [1.8],\n",
       "       [1.3],\n",
       "       [1.5],\n",
       "       [1.2],\n",
       "       [1.3],\n",
       "       [1.4],\n",
       "       [1.4],\n",
       "       [1.7],\n",
       "       [1.5],\n",
       "       [1. ],\n",
       "       [1.1],\n",
       "       [1. ],\n",
       "       [1.2],\n",
       "       [1.6],\n",
       "       [1.5],\n",
       "       [1.6],\n",
       "       [1.5],\n",
       "       [1.3],\n",
       "       [1.3],\n",
       "       [1.3],\n",
       "       [1.2],\n",
       "       [1.4],\n",
       "       [1.2],\n",
       "       [1. ],\n",
       "       [1.3],\n",
       "       [1.2],\n",
       "       [1.3],\n",
       "       [1.3],\n",
       "       [1.1],\n",
       "       [1.3],\n",
       "       [2.5],\n",
       "       [1.9],\n",
       "       [2.1],\n",
       "       [1.8],\n",
       "       [2.2],\n",
       "       [2.1],\n",
       "       [1.7],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.5],\n",
       "       [2. ],\n",
       "       [1.9],\n",
       "       [2.1],\n",
       "       [2. ],\n",
       "       [2.4],\n",
       "       [2.3],\n",
       "       [1.8],\n",
       "       [2.2],\n",
       "       [2.3],\n",
       "       [1.5],\n",
       "       [2.3],\n",
       "       [2. ],\n",
       "       [2. ],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [1.6],\n",
       "       [1.9],\n",
       "       [2. ],\n",
       "       [2.2],\n",
       "       [1.5],\n",
       "       [1.4],\n",
       "       [2.3],\n",
       "       [2.4],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [2.4],\n",
       "       [2.3],\n",
       "       [1.9],\n",
       "       [2.3],\n",
       "       [2.5],\n",
       "       [2.3],\n",
       "       [1.9],\n",
       "       [2. ],\n",
       "       [2.3],\n",
       "       [1.8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02cea2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac3e1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= (iris['target'] == 2)  #if target==2 then it will return (true)1, else (false)0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "900d3303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a54d32b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d77a72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = clf.predict(([[2.6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0873469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c613fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = clf.predict(([[1.6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1baf6b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n"
     ]
    }
   ],
   "source": [
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8215a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using matplotlib to plot the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e473d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new=np.linspace(0,4,1000).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01e65c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.004004  ],\n",
       "       [0.00800801],\n",
       "       [0.01201201],\n",
       "       [0.01601602],\n",
       "       [0.02002002],\n",
       "       [0.02402402],\n",
       "       [0.02802803],\n",
       "       [0.03203203],\n",
       "       [0.03603604],\n",
       "       [0.04004004],\n",
       "       [0.04404404],\n",
       "       [0.04804805],\n",
       "       [0.05205205],\n",
       "       [0.05605606],\n",
       "       [0.06006006],\n",
       "       [0.06406406],\n",
       "       [0.06806807],\n",
       "       [0.07207207],\n",
       "       [0.07607608],\n",
       "       [0.08008008],\n",
       "       [0.08408408],\n",
       "       [0.08808809],\n",
       "       [0.09209209],\n",
       "       [0.0960961 ],\n",
       "       [0.1001001 ],\n",
       "       [0.1041041 ],\n",
       "       [0.10810811],\n",
       "       [0.11211211],\n",
       "       [0.11611612],\n",
       "       [0.12012012],\n",
       "       [0.12412412],\n",
       "       [0.12812813],\n",
       "       [0.13213213],\n",
       "       [0.13613614],\n",
       "       [0.14014014],\n",
       "       [0.14414414],\n",
       "       [0.14814815],\n",
       "       [0.15215215],\n",
       "       [0.15615616],\n",
       "       [0.16016016],\n",
       "       [0.16416416],\n",
       "       [0.16816817],\n",
       "       [0.17217217],\n",
       "       [0.17617618],\n",
       "       [0.18018018],\n",
       "       [0.18418418],\n",
       "       [0.18818819],\n",
       "       [0.19219219],\n",
       "       [0.1961962 ],\n",
       "       [0.2002002 ],\n",
       "       [0.2042042 ],\n",
       "       [0.20820821],\n",
       "       [0.21221221],\n",
       "       [0.21621622],\n",
       "       [0.22022022],\n",
       "       [0.22422422],\n",
       "       [0.22822823],\n",
       "       [0.23223223],\n",
       "       [0.23623624],\n",
       "       [0.24024024],\n",
       "       [0.24424424],\n",
       "       [0.24824825],\n",
       "       [0.25225225],\n",
       "       [0.25625626],\n",
       "       [0.26026026],\n",
       "       [0.26426426],\n",
       "       [0.26826827],\n",
       "       [0.27227227],\n",
       "       [0.27627628],\n",
       "       [0.28028028],\n",
       "       [0.28428428],\n",
       "       [0.28828829],\n",
       "       [0.29229229],\n",
       "       [0.2962963 ],\n",
       "       [0.3003003 ],\n",
       "       [0.3043043 ],\n",
       "       [0.30830831],\n",
       "       [0.31231231],\n",
       "       [0.31631632],\n",
       "       [0.32032032],\n",
       "       [0.32432432],\n",
       "       [0.32832833],\n",
       "       [0.33233233],\n",
       "       [0.33633634],\n",
       "       [0.34034034],\n",
       "       [0.34434434],\n",
       "       [0.34834835],\n",
       "       [0.35235235],\n",
       "       [0.35635636],\n",
       "       [0.36036036],\n",
       "       [0.36436436],\n",
       "       [0.36836837],\n",
       "       [0.37237237],\n",
       "       [0.37637638],\n",
       "       [0.38038038],\n",
       "       [0.38438438],\n",
       "       [0.38838839],\n",
       "       [0.39239239],\n",
       "       [0.3963964 ],\n",
       "       [0.4004004 ],\n",
       "       [0.4044044 ],\n",
       "       [0.40840841],\n",
       "       [0.41241241],\n",
       "       [0.41641642],\n",
       "       [0.42042042],\n",
       "       [0.42442442],\n",
       "       [0.42842843],\n",
       "       [0.43243243],\n",
       "       [0.43643644],\n",
       "       [0.44044044],\n",
       "       [0.44444444],\n",
       "       [0.44844845],\n",
       "       [0.45245245],\n",
       "       [0.45645646],\n",
       "       [0.46046046],\n",
       "       [0.46446446],\n",
       "       [0.46846847],\n",
       "       [0.47247247],\n",
       "       [0.47647648],\n",
       "       [0.48048048],\n",
       "       [0.48448448],\n",
       "       [0.48848849],\n",
       "       [0.49249249],\n",
       "       [0.4964965 ],\n",
       "       [0.5005005 ],\n",
       "       [0.5045045 ],\n",
       "       [0.50850851],\n",
       "       [0.51251251],\n",
       "       [0.51651652],\n",
       "       [0.52052052],\n",
       "       [0.52452452],\n",
       "       [0.52852853],\n",
       "       [0.53253253],\n",
       "       [0.53653654],\n",
       "       [0.54054054],\n",
       "       [0.54454454],\n",
       "       [0.54854855],\n",
       "       [0.55255255],\n",
       "       [0.55655656],\n",
       "       [0.56056056],\n",
       "       [0.56456456],\n",
       "       [0.56856857],\n",
       "       [0.57257257],\n",
       "       [0.57657658],\n",
       "       [0.58058058],\n",
       "       [0.58458458],\n",
       "       [0.58858859],\n",
       "       [0.59259259],\n",
       "       [0.5965966 ],\n",
       "       [0.6006006 ],\n",
       "       [0.6046046 ],\n",
       "       [0.60860861],\n",
       "       [0.61261261],\n",
       "       [0.61661662],\n",
       "       [0.62062062],\n",
       "       [0.62462462],\n",
       "       [0.62862863],\n",
       "       [0.63263263],\n",
       "       [0.63663664],\n",
       "       [0.64064064],\n",
       "       [0.64464464],\n",
       "       [0.64864865],\n",
       "       [0.65265265],\n",
       "       [0.65665666],\n",
       "       [0.66066066],\n",
       "       [0.66466466],\n",
       "       [0.66866867],\n",
       "       [0.67267267],\n",
       "       [0.67667668],\n",
       "       [0.68068068],\n",
       "       [0.68468468],\n",
       "       [0.68868869],\n",
       "       [0.69269269],\n",
       "       [0.6966967 ],\n",
       "       [0.7007007 ],\n",
       "       [0.7047047 ],\n",
       "       [0.70870871],\n",
       "       [0.71271271],\n",
       "       [0.71671672],\n",
       "       [0.72072072],\n",
       "       [0.72472472],\n",
       "       [0.72872873],\n",
       "       [0.73273273],\n",
       "       [0.73673674],\n",
       "       [0.74074074],\n",
       "       [0.74474474],\n",
       "       [0.74874875],\n",
       "       [0.75275275],\n",
       "       [0.75675676],\n",
       "       [0.76076076],\n",
       "       [0.76476476],\n",
       "       [0.76876877],\n",
       "       [0.77277277],\n",
       "       [0.77677678],\n",
       "       [0.78078078],\n",
       "       [0.78478478],\n",
       "       [0.78878879],\n",
       "       [0.79279279],\n",
       "       [0.7967968 ],\n",
       "       [0.8008008 ],\n",
       "       [0.8048048 ],\n",
       "       [0.80880881],\n",
       "       [0.81281281],\n",
       "       [0.81681682],\n",
       "       [0.82082082],\n",
       "       [0.82482482],\n",
       "       [0.82882883],\n",
       "       [0.83283283],\n",
       "       [0.83683684],\n",
       "       [0.84084084],\n",
       "       [0.84484484],\n",
       "       [0.84884885],\n",
       "       [0.85285285],\n",
       "       [0.85685686],\n",
       "       [0.86086086],\n",
       "       [0.86486486],\n",
       "       [0.86886887],\n",
       "       [0.87287287],\n",
       "       [0.87687688],\n",
       "       [0.88088088],\n",
       "       [0.88488488],\n",
       "       [0.88888889],\n",
       "       [0.89289289],\n",
       "       [0.8968969 ],\n",
       "       [0.9009009 ],\n",
       "       [0.9049049 ],\n",
       "       [0.90890891],\n",
       "       [0.91291291],\n",
       "       [0.91691692],\n",
       "       [0.92092092],\n",
       "       [0.92492492],\n",
       "       [0.92892893],\n",
       "       [0.93293293],\n",
       "       [0.93693694],\n",
       "       [0.94094094],\n",
       "       [0.94494494],\n",
       "       [0.94894895],\n",
       "       [0.95295295],\n",
       "       [0.95695696],\n",
       "       [0.96096096],\n",
       "       [0.96496496],\n",
       "       [0.96896897],\n",
       "       [0.97297297],\n",
       "       [0.97697698],\n",
       "       [0.98098098],\n",
       "       [0.98498498],\n",
       "       [0.98898899],\n",
       "       [0.99299299],\n",
       "       [0.996997  ],\n",
       "       [1.001001  ],\n",
       "       [1.00500501],\n",
       "       [1.00900901],\n",
       "       [1.01301301],\n",
       "       [1.01701702],\n",
       "       [1.02102102],\n",
       "       [1.02502503],\n",
       "       [1.02902903],\n",
       "       [1.03303303],\n",
       "       [1.03703704],\n",
       "       [1.04104104],\n",
       "       [1.04504505],\n",
       "       [1.04904905],\n",
       "       [1.05305305],\n",
       "       [1.05705706],\n",
       "       [1.06106106],\n",
       "       [1.06506507],\n",
       "       [1.06906907],\n",
       "       [1.07307307],\n",
       "       [1.07707708],\n",
       "       [1.08108108],\n",
       "       [1.08508509],\n",
       "       [1.08908909],\n",
       "       [1.09309309],\n",
       "       [1.0970971 ],\n",
       "       [1.1011011 ],\n",
       "       [1.10510511],\n",
       "       [1.10910911],\n",
       "       [1.11311311],\n",
       "       [1.11711712],\n",
       "       [1.12112112],\n",
       "       [1.12512513],\n",
       "       [1.12912913],\n",
       "       [1.13313313],\n",
       "       [1.13713714],\n",
       "       [1.14114114],\n",
       "       [1.14514515],\n",
       "       [1.14914915],\n",
       "       [1.15315315],\n",
       "       [1.15715716],\n",
       "       [1.16116116],\n",
       "       [1.16516517],\n",
       "       [1.16916917],\n",
       "       [1.17317317],\n",
       "       [1.17717718],\n",
       "       [1.18118118],\n",
       "       [1.18518519],\n",
       "       [1.18918919],\n",
       "       [1.19319319],\n",
       "       [1.1971972 ],\n",
       "       [1.2012012 ],\n",
       "       [1.20520521],\n",
       "       [1.20920921],\n",
       "       [1.21321321],\n",
       "       [1.21721722],\n",
       "       [1.22122122],\n",
       "       [1.22522523],\n",
       "       [1.22922923],\n",
       "       [1.23323323],\n",
       "       [1.23723724],\n",
       "       [1.24124124],\n",
       "       [1.24524525],\n",
       "       [1.24924925],\n",
       "       [1.25325325],\n",
       "       [1.25725726],\n",
       "       [1.26126126],\n",
       "       [1.26526527],\n",
       "       [1.26926927],\n",
       "       [1.27327327],\n",
       "       [1.27727728],\n",
       "       [1.28128128],\n",
       "       [1.28528529],\n",
       "       [1.28928929],\n",
       "       [1.29329329],\n",
       "       [1.2972973 ],\n",
       "       [1.3013013 ],\n",
       "       [1.30530531],\n",
       "       [1.30930931],\n",
       "       [1.31331331],\n",
       "       [1.31731732],\n",
       "       [1.32132132],\n",
       "       [1.32532533],\n",
       "       [1.32932933],\n",
       "       [1.33333333],\n",
       "       [1.33733734],\n",
       "       [1.34134134],\n",
       "       [1.34534535],\n",
       "       [1.34934935],\n",
       "       [1.35335335],\n",
       "       [1.35735736],\n",
       "       [1.36136136],\n",
       "       [1.36536537],\n",
       "       [1.36936937],\n",
       "       [1.37337337],\n",
       "       [1.37737738],\n",
       "       [1.38138138],\n",
       "       [1.38538539],\n",
       "       [1.38938939],\n",
       "       [1.39339339],\n",
       "       [1.3973974 ],\n",
       "       [1.4014014 ],\n",
       "       [1.40540541],\n",
       "       [1.40940941],\n",
       "       [1.41341341],\n",
       "       [1.41741742],\n",
       "       [1.42142142],\n",
       "       [1.42542543],\n",
       "       [1.42942943],\n",
       "       [1.43343343],\n",
       "       [1.43743744],\n",
       "       [1.44144144],\n",
       "       [1.44544545],\n",
       "       [1.44944945],\n",
       "       [1.45345345],\n",
       "       [1.45745746],\n",
       "       [1.46146146],\n",
       "       [1.46546547],\n",
       "       [1.46946947],\n",
       "       [1.47347347],\n",
       "       [1.47747748],\n",
       "       [1.48148148],\n",
       "       [1.48548549],\n",
       "       [1.48948949],\n",
       "       [1.49349349],\n",
       "       [1.4974975 ],\n",
       "       [1.5015015 ],\n",
       "       [1.50550551],\n",
       "       [1.50950951],\n",
       "       [1.51351351],\n",
       "       [1.51751752],\n",
       "       [1.52152152],\n",
       "       [1.52552553],\n",
       "       [1.52952953],\n",
       "       [1.53353353],\n",
       "       [1.53753754],\n",
       "       [1.54154154],\n",
       "       [1.54554555],\n",
       "       [1.54954955],\n",
       "       [1.55355355],\n",
       "       [1.55755756],\n",
       "       [1.56156156],\n",
       "       [1.56556557],\n",
       "       [1.56956957],\n",
       "       [1.57357357],\n",
       "       [1.57757758],\n",
       "       [1.58158158],\n",
       "       [1.58558559],\n",
       "       [1.58958959],\n",
       "       [1.59359359],\n",
       "       [1.5975976 ],\n",
       "       [1.6016016 ],\n",
       "       [1.60560561],\n",
       "       [1.60960961],\n",
       "       [1.61361361],\n",
       "       [1.61761762],\n",
       "       [1.62162162],\n",
       "       [1.62562563],\n",
       "       [1.62962963],\n",
       "       [1.63363363],\n",
       "       [1.63763764],\n",
       "       [1.64164164],\n",
       "       [1.64564565],\n",
       "       [1.64964965],\n",
       "       [1.65365365],\n",
       "       [1.65765766],\n",
       "       [1.66166166],\n",
       "       [1.66566567],\n",
       "       [1.66966967],\n",
       "       [1.67367367],\n",
       "       [1.67767768],\n",
       "       [1.68168168],\n",
       "       [1.68568569],\n",
       "       [1.68968969],\n",
       "       [1.69369369],\n",
       "       [1.6976977 ],\n",
       "       [1.7017017 ],\n",
       "       [1.70570571],\n",
       "       [1.70970971],\n",
       "       [1.71371371],\n",
       "       [1.71771772],\n",
       "       [1.72172172],\n",
       "       [1.72572573],\n",
       "       [1.72972973],\n",
       "       [1.73373373],\n",
       "       [1.73773774],\n",
       "       [1.74174174],\n",
       "       [1.74574575],\n",
       "       [1.74974975],\n",
       "       [1.75375375],\n",
       "       [1.75775776],\n",
       "       [1.76176176],\n",
       "       [1.76576577],\n",
       "       [1.76976977],\n",
       "       [1.77377377],\n",
       "       [1.77777778],\n",
       "       [1.78178178],\n",
       "       [1.78578579],\n",
       "       [1.78978979],\n",
       "       [1.79379379],\n",
       "       [1.7977978 ],\n",
       "       [1.8018018 ],\n",
       "       [1.80580581],\n",
       "       [1.80980981],\n",
       "       [1.81381381],\n",
       "       [1.81781782],\n",
       "       [1.82182182],\n",
       "       [1.82582583],\n",
       "       [1.82982983],\n",
       "       [1.83383383],\n",
       "       [1.83783784],\n",
       "       [1.84184184],\n",
       "       [1.84584585],\n",
       "       [1.84984985],\n",
       "       [1.85385385],\n",
       "       [1.85785786],\n",
       "       [1.86186186],\n",
       "       [1.86586587],\n",
       "       [1.86986987],\n",
       "       [1.87387387],\n",
       "       [1.87787788],\n",
       "       [1.88188188],\n",
       "       [1.88588589],\n",
       "       [1.88988989],\n",
       "       [1.89389389],\n",
       "       [1.8978979 ],\n",
       "       [1.9019019 ],\n",
       "       [1.90590591],\n",
       "       [1.90990991],\n",
       "       [1.91391391],\n",
       "       [1.91791792],\n",
       "       [1.92192192],\n",
       "       [1.92592593],\n",
       "       [1.92992993],\n",
       "       [1.93393393],\n",
       "       [1.93793794],\n",
       "       [1.94194194],\n",
       "       [1.94594595],\n",
       "       [1.94994995],\n",
       "       [1.95395395],\n",
       "       [1.95795796],\n",
       "       [1.96196196],\n",
       "       [1.96596597],\n",
       "       [1.96996997],\n",
       "       [1.97397397],\n",
       "       [1.97797798],\n",
       "       [1.98198198],\n",
       "       [1.98598599],\n",
       "       [1.98998999],\n",
       "       [1.99399399],\n",
       "       [1.997998  ],\n",
       "       [2.002002  ],\n",
       "       [2.00600601],\n",
       "       [2.01001001],\n",
       "       [2.01401401],\n",
       "       [2.01801802],\n",
       "       [2.02202202],\n",
       "       [2.02602603],\n",
       "       [2.03003003],\n",
       "       [2.03403403],\n",
       "       [2.03803804],\n",
       "       [2.04204204],\n",
       "       [2.04604605],\n",
       "       [2.05005005],\n",
       "       [2.05405405],\n",
       "       [2.05805806],\n",
       "       [2.06206206],\n",
       "       [2.06606607],\n",
       "       [2.07007007],\n",
       "       [2.07407407],\n",
       "       [2.07807808],\n",
       "       [2.08208208],\n",
       "       [2.08608609],\n",
       "       [2.09009009],\n",
       "       [2.09409409],\n",
       "       [2.0980981 ],\n",
       "       [2.1021021 ],\n",
       "       [2.10610611],\n",
       "       [2.11011011],\n",
       "       [2.11411411],\n",
       "       [2.11811812],\n",
       "       [2.12212212],\n",
       "       [2.12612613],\n",
       "       [2.13013013],\n",
       "       [2.13413413],\n",
       "       [2.13813814],\n",
       "       [2.14214214],\n",
       "       [2.14614615],\n",
       "       [2.15015015],\n",
       "       [2.15415415],\n",
       "       [2.15815816],\n",
       "       [2.16216216],\n",
       "       [2.16616617],\n",
       "       [2.17017017],\n",
       "       [2.17417417],\n",
       "       [2.17817818],\n",
       "       [2.18218218],\n",
       "       [2.18618619],\n",
       "       [2.19019019],\n",
       "       [2.19419419],\n",
       "       [2.1981982 ],\n",
       "       [2.2022022 ],\n",
       "       [2.20620621],\n",
       "       [2.21021021],\n",
       "       [2.21421421],\n",
       "       [2.21821822],\n",
       "       [2.22222222],\n",
       "       [2.22622623],\n",
       "       [2.23023023],\n",
       "       [2.23423423],\n",
       "       [2.23823824],\n",
       "       [2.24224224],\n",
       "       [2.24624625],\n",
       "       [2.25025025],\n",
       "       [2.25425425],\n",
       "       [2.25825826],\n",
       "       [2.26226226],\n",
       "       [2.26626627],\n",
       "       [2.27027027],\n",
       "       [2.27427427],\n",
       "       [2.27827828],\n",
       "       [2.28228228],\n",
       "       [2.28628629],\n",
       "       [2.29029029],\n",
       "       [2.29429429],\n",
       "       [2.2982983 ],\n",
       "       [2.3023023 ],\n",
       "       [2.30630631],\n",
       "       [2.31031031],\n",
       "       [2.31431431],\n",
       "       [2.31831832],\n",
       "       [2.32232232],\n",
       "       [2.32632633],\n",
       "       [2.33033033],\n",
       "       [2.33433433],\n",
       "       [2.33833834],\n",
       "       [2.34234234],\n",
       "       [2.34634635],\n",
       "       [2.35035035],\n",
       "       [2.35435435],\n",
       "       [2.35835836],\n",
       "       [2.36236236],\n",
       "       [2.36636637],\n",
       "       [2.37037037],\n",
       "       [2.37437437],\n",
       "       [2.37837838],\n",
       "       [2.38238238],\n",
       "       [2.38638639],\n",
       "       [2.39039039],\n",
       "       [2.39439439],\n",
       "       [2.3983984 ],\n",
       "       [2.4024024 ],\n",
       "       [2.40640641],\n",
       "       [2.41041041],\n",
       "       [2.41441441],\n",
       "       [2.41841842],\n",
       "       [2.42242242],\n",
       "       [2.42642643],\n",
       "       [2.43043043],\n",
       "       [2.43443443],\n",
       "       [2.43843844],\n",
       "       [2.44244244],\n",
       "       [2.44644645],\n",
       "       [2.45045045],\n",
       "       [2.45445445],\n",
       "       [2.45845846],\n",
       "       [2.46246246],\n",
       "       [2.46646647],\n",
       "       [2.47047047],\n",
       "       [2.47447447],\n",
       "       [2.47847848],\n",
       "       [2.48248248],\n",
       "       [2.48648649],\n",
       "       [2.49049049],\n",
       "       [2.49449449],\n",
       "       [2.4984985 ],\n",
       "       [2.5025025 ],\n",
       "       [2.50650651],\n",
       "       [2.51051051],\n",
       "       [2.51451451],\n",
       "       [2.51851852],\n",
       "       [2.52252252],\n",
       "       [2.52652653],\n",
       "       [2.53053053],\n",
       "       [2.53453453],\n",
       "       [2.53853854],\n",
       "       [2.54254254],\n",
       "       [2.54654655],\n",
       "       [2.55055055],\n",
       "       [2.55455455],\n",
       "       [2.55855856],\n",
       "       [2.56256256],\n",
       "       [2.56656657],\n",
       "       [2.57057057],\n",
       "       [2.57457457],\n",
       "       [2.57857858],\n",
       "       [2.58258258],\n",
       "       [2.58658659],\n",
       "       [2.59059059],\n",
       "       [2.59459459],\n",
       "       [2.5985986 ],\n",
       "       [2.6026026 ],\n",
       "       [2.60660661],\n",
       "       [2.61061061],\n",
       "       [2.61461461],\n",
       "       [2.61861862],\n",
       "       [2.62262262],\n",
       "       [2.62662663],\n",
       "       [2.63063063],\n",
       "       [2.63463463],\n",
       "       [2.63863864],\n",
       "       [2.64264264],\n",
       "       [2.64664665],\n",
       "       [2.65065065],\n",
       "       [2.65465465],\n",
       "       [2.65865866],\n",
       "       [2.66266266],\n",
       "       [2.66666667],\n",
       "       [2.67067067],\n",
       "       [2.67467467],\n",
       "       [2.67867868],\n",
       "       [2.68268268],\n",
       "       [2.68668669],\n",
       "       [2.69069069],\n",
       "       [2.69469469],\n",
       "       [2.6986987 ],\n",
       "       [2.7027027 ],\n",
       "       [2.70670671],\n",
       "       [2.71071071],\n",
       "       [2.71471471],\n",
       "       [2.71871872],\n",
       "       [2.72272272],\n",
       "       [2.72672673],\n",
       "       [2.73073073],\n",
       "       [2.73473473],\n",
       "       [2.73873874],\n",
       "       [2.74274274],\n",
       "       [2.74674675],\n",
       "       [2.75075075],\n",
       "       [2.75475475],\n",
       "       [2.75875876],\n",
       "       [2.76276276],\n",
       "       [2.76676677],\n",
       "       [2.77077077],\n",
       "       [2.77477477],\n",
       "       [2.77877878],\n",
       "       [2.78278278],\n",
       "       [2.78678679],\n",
       "       [2.79079079],\n",
       "       [2.79479479],\n",
       "       [2.7987988 ],\n",
       "       [2.8028028 ],\n",
       "       [2.80680681],\n",
       "       [2.81081081],\n",
       "       [2.81481481],\n",
       "       [2.81881882],\n",
       "       [2.82282282],\n",
       "       [2.82682683],\n",
       "       [2.83083083],\n",
       "       [2.83483483],\n",
       "       [2.83883884],\n",
       "       [2.84284284],\n",
       "       [2.84684685],\n",
       "       [2.85085085],\n",
       "       [2.85485485],\n",
       "       [2.85885886],\n",
       "       [2.86286286],\n",
       "       [2.86686687],\n",
       "       [2.87087087],\n",
       "       [2.87487487],\n",
       "       [2.87887888],\n",
       "       [2.88288288],\n",
       "       [2.88688689],\n",
       "       [2.89089089],\n",
       "       [2.89489489],\n",
       "       [2.8988989 ],\n",
       "       [2.9029029 ],\n",
       "       [2.90690691],\n",
       "       [2.91091091],\n",
       "       [2.91491491],\n",
       "       [2.91891892],\n",
       "       [2.92292292],\n",
       "       [2.92692693],\n",
       "       [2.93093093],\n",
       "       [2.93493493],\n",
       "       [2.93893894],\n",
       "       [2.94294294],\n",
       "       [2.94694695],\n",
       "       [2.95095095],\n",
       "       [2.95495495],\n",
       "       [2.95895896],\n",
       "       [2.96296296],\n",
       "       [2.96696697],\n",
       "       [2.97097097],\n",
       "       [2.97497497],\n",
       "       [2.97897898],\n",
       "       [2.98298298],\n",
       "       [2.98698699],\n",
       "       [2.99099099],\n",
       "       [2.99499499],\n",
       "       [2.998999  ],\n",
       "       [3.003003  ],\n",
       "       [3.00700701],\n",
       "       [3.01101101],\n",
       "       [3.01501502],\n",
       "       [3.01901902],\n",
       "       [3.02302302],\n",
       "       [3.02702703],\n",
       "       [3.03103103],\n",
       "       [3.03503504],\n",
       "       [3.03903904],\n",
       "       [3.04304304],\n",
       "       [3.04704705],\n",
       "       [3.05105105],\n",
       "       [3.05505506],\n",
       "       [3.05905906],\n",
       "       [3.06306306],\n",
       "       [3.06706707],\n",
       "       [3.07107107],\n",
       "       [3.07507508],\n",
       "       [3.07907908],\n",
       "       [3.08308308],\n",
       "       [3.08708709],\n",
       "       [3.09109109],\n",
       "       [3.0950951 ],\n",
       "       [3.0990991 ],\n",
       "       [3.1031031 ],\n",
       "       [3.10710711],\n",
       "       [3.11111111],\n",
       "       [3.11511512],\n",
       "       [3.11911912],\n",
       "       [3.12312312],\n",
       "       [3.12712713],\n",
       "       [3.13113113],\n",
       "       [3.13513514],\n",
       "       [3.13913914],\n",
       "       [3.14314314],\n",
       "       [3.14714715],\n",
       "       [3.15115115],\n",
       "       [3.15515516],\n",
       "       [3.15915916],\n",
       "       [3.16316316],\n",
       "       [3.16716717],\n",
       "       [3.17117117],\n",
       "       [3.17517518],\n",
       "       [3.17917918],\n",
       "       [3.18318318],\n",
       "       [3.18718719],\n",
       "       [3.19119119],\n",
       "       [3.1951952 ],\n",
       "       [3.1991992 ],\n",
       "       [3.2032032 ],\n",
       "       [3.20720721],\n",
       "       [3.21121121],\n",
       "       [3.21521522],\n",
       "       [3.21921922],\n",
       "       [3.22322322],\n",
       "       [3.22722723],\n",
       "       [3.23123123],\n",
       "       [3.23523524],\n",
       "       [3.23923924],\n",
       "       [3.24324324],\n",
       "       [3.24724725],\n",
       "       [3.25125125],\n",
       "       [3.25525526],\n",
       "       [3.25925926],\n",
       "       [3.26326326],\n",
       "       [3.26726727],\n",
       "       [3.27127127],\n",
       "       [3.27527528],\n",
       "       [3.27927928],\n",
       "       [3.28328328],\n",
       "       [3.28728729],\n",
       "       [3.29129129],\n",
       "       [3.2952953 ],\n",
       "       [3.2992993 ],\n",
       "       [3.3033033 ],\n",
       "       [3.30730731],\n",
       "       [3.31131131],\n",
       "       [3.31531532],\n",
       "       [3.31931932],\n",
       "       [3.32332332],\n",
       "       [3.32732733],\n",
       "       [3.33133133],\n",
       "       [3.33533534],\n",
       "       [3.33933934],\n",
       "       [3.34334334],\n",
       "       [3.34734735],\n",
       "       [3.35135135],\n",
       "       [3.35535536],\n",
       "       [3.35935936],\n",
       "       [3.36336336],\n",
       "       [3.36736737],\n",
       "       [3.37137137],\n",
       "       [3.37537538],\n",
       "       [3.37937938],\n",
       "       [3.38338338],\n",
       "       [3.38738739],\n",
       "       [3.39139139],\n",
       "       [3.3953954 ],\n",
       "       [3.3993994 ],\n",
       "       [3.4034034 ],\n",
       "       [3.40740741],\n",
       "       [3.41141141],\n",
       "       [3.41541542],\n",
       "       [3.41941942],\n",
       "       [3.42342342],\n",
       "       [3.42742743],\n",
       "       [3.43143143],\n",
       "       [3.43543544],\n",
       "       [3.43943944],\n",
       "       [3.44344344],\n",
       "       [3.44744745],\n",
       "       [3.45145145],\n",
       "       [3.45545546],\n",
       "       [3.45945946],\n",
       "       [3.46346346],\n",
       "       [3.46746747],\n",
       "       [3.47147147],\n",
       "       [3.47547548],\n",
       "       [3.47947948],\n",
       "       [3.48348348],\n",
       "       [3.48748749],\n",
       "       [3.49149149],\n",
       "       [3.4954955 ],\n",
       "       [3.4994995 ],\n",
       "       [3.5035035 ],\n",
       "       [3.50750751],\n",
       "       [3.51151151],\n",
       "       [3.51551552],\n",
       "       [3.51951952],\n",
       "       [3.52352352],\n",
       "       [3.52752753],\n",
       "       [3.53153153],\n",
       "       [3.53553554],\n",
       "       [3.53953954],\n",
       "       [3.54354354],\n",
       "       [3.54754755],\n",
       "       [3.55155155],\n",
       "       [3.55555556],\n",
       "       [3.55955956],\n",
       "       [3.56356356],\n",
       "       [3.56756757],\n",
       "       [3.57157157],\n",
       "       [3.57557558],\n",
       "       [3.57957958],\n",
       "       [3.58358358],\n",
       "       [3.58758759],\n",
       "       [3.59159159],\n",
       "       [3.5955956 ],\n",
       "       [3.5995996 ],\n",
       "       [3.6036036 ],\n",
       "       [3.60760761],\n",
       "       [3.61161161],\n",
       "       [3.61561562],\n",
       "       [3.61961962],\n",
       "       [3.62362362],\n",
       "       [3.62762763],\n",
       "       [3.63163163],\n",
       "       [3.63563564],\n",
       "       [3.63963964],\n",
       "       [3.64364364],\n",
       "       [3.64764765],\n",
       "       [3.65165165],\n",
       "       [3.65565566],\n",
       "       [3.65965966],\n",
       "       [3.66366366],\n",
       "       [3.66766767],\n",
       "       [3.67167167],\n",
       "       [3.67567568],\n",
       "       [3.67967968],\n",
       "       [3.68368368],\n",
       "       [3.68768769],\n",
       "       [3.69169169],\n",
       "       [3.6956957 ],\n",
       "       [3.6996997 ],\n",
       "       [3.7037037 ],\n",
       "       [3.70770771],\n",
       "       [3.71171171],\n",
       "       [3.71571572],\n",
       "       [3.71971972],\n",
       "       [3.72372372],\n",
       "       [3.72772773],\n",
       "       [3.73173173],\n",
       "       [3.73573574],\n",
       "       [3.73973974],\n",
       "       [3.74374374],\n",
       "       [3.74774775],\n",
       "       [3.75175175],\n",
       "       [3.75575576],\n",
       "       [3.75975976],\n",
       "       [3.76376376],\n",
       "       [3.76776777],\n",
       "       [3.77177177],\n",
       "       [3.77577578],\n",
       "       [3.77977978],\n",
       "       [3.78378378],\n",
       "       [3.78778779],\n",
       "       [3.79179179],\n",
       "       [3.7957958 ],\n",
       "       [3.7997998 ],\n",
       "       [3.8038038 ],\n",
       "       [3.80780781],\n",
       "       [3.81181181],\n",
       "       [3.81581582],\n",
       "       [3.81981982],\n",
       "       [3.82382382],\n",
       "       [3.82782783],\n",
       "       [3.83183183],\n",
       "       [3.83583584],\n",
       "       [3.83983984],\n",
       "       [3.84384384],\n",
       "       [3.84784785],\n",
       "       [3.85185185],\n",
       "       [3.85585586],\n",
       "       [3.85985986],\n",
       "       [3.86386386],\n",
       "       [3.86786787],\n",
       "       [3.87187187],\n",
       "       [3.87587588],\n",
       "       [3.87987988],\n",
       "       [3.88388388],\n",
       "       [3.88788789],\n",
       "       [3.89189189],\n",
       "       [3.8958959 ],\n",
       "       [3.8998999 ],\n",
       "       [3.9039039 ],\n",
       "       [3.90790791],\n",
       "       [3.91191191],\n",
       "       [3.91591592],\n",
       "       [3.91991992],\n",
       "       [3.92392392],\n",
       "       [3.92792793],\n",
       "       [3.93193193],\n",
       "       [3.93593594],\n",
       "       [3.93993994],\n",
       "       [3.94394394],\n",
       "       [3.94794795],\n",
       "       [3.95195195],\n",
       "       [3.95595596],\n",
       "       [3.95995996],\n",
       "       [3.96396396],\n",
       "       [3.96796797],\n",
       "       [3.97197197],\n",
       "       [3.97597598],\n",
       "       [3.97997998],\n",
       "       [3.98398398],\n",
       "       [3.98798799],\n",
       "       [3.99199199],\n",
       "       [3.995996  ],\n",
       "       [4.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a93d32e-accc-4eab-917c-a6259f3d7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probablity=clf.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ef67991-06c4-4fbc-bfaa-1d94c9ddcd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f4c9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probablity=clf.predict_proba(x_new)  #will get probablity values by using predict.proba & will get 1 or 0 if used only predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37db0fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99250016e-01, 7.49984089e-04],\n",
       "       [9.99236900e-01, 7.63099595e-04],\n",
       "       [9.99223556e-01, 7.76444284e-04],\n",
       "       ...,\n",
       "       [4.09576617e-05, 9.99959042e-01],\n",
       "       [4.02532162e-05, 9.99959747e-01],\n",
       "       [3.95608863e-05, 9.99960439e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77716c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_new, y_probablity[:,1], \"blue\", label=\"setosa\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ddd1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1475560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import datasets\n",
    "df=datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c23b9db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n  for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n  Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n  San Jose, CA, 1993.\\n- O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n  prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n  July-August 1995.\\n- W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n  to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n  163-171.\\n\\n|details-end|',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'breast_cancer.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2aa9f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(df['data'],columns=df['feature_names'])    #independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1a17046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10ef31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame(df['target'],columns=['Target'])    #dependent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6df9919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target\n",
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "..      ...\n",
       "564       0\n",
       "565       0\n",
       "566       0\n",
       "567       0\n",
       "568       1\n",
       "\n",
       "[569 rows x 1 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b13abe11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "1    357\n",
       "0    212\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f816dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4625f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params=[{'C':[1,5,10]},{'max_iter':[100,150]}]\n",
    "model1=LogisticRegression(C=100,max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1b14997",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GridSearchCV(model1,param_grid=params,scoring='f1',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5666bf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(C=100),\n",
       "             param_grid=[{&#x27;C&#x27;: [1, 5, 10]}, {&#x27;max_iter&#x27;: [100, 150]}],\n",
       "             scoring=&#x27;f1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(C=100),\n",
       "             param_grid=[{&#x27;C&#x27;: [1, 5, 10]}, {&#x27;max_iter&#x27;: [100, 150]}],\n",
       "             scoring=&#x27;f1&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(C=100),\n",
       "             param_grid=[{'C': [1, 5, 10]}, {'max_iter': [100, 150]}],\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e414477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_iter': 150}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d60fec5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9515602774811989"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcbfff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c73ddfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df56b85f-1204-4044-8c39-e821f57b7b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target\n",
       "204       1\n",
       "70        0\n",
       "131       0\n",
       "431       1\n",
       "540       1\n",
       "..      ...\n",
       "141       0\n",
       "498       0\n",
       "7         0\n",
       "541       1\n",
       "19        1\n",
       "\n",
       "[188 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb729e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7a3961e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 64,   3],\n",
       "       [  3, 118]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba7db107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        67\n",
      "           1       0.98      0.98      0.98       121\n",
      "\n",
      "    accuracy                           0.97       188\n",
      "   macro avg       0.97      0.97      0.97       188\n",
      "weighted avg       0.97      0.97      0.97       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2137adbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9680851063829787"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476d7be-d88d-4e5a-84bf-84e1338a44e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
